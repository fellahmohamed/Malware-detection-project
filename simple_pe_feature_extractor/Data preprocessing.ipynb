{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "413316bc",
   "metadata": {},
   "source": [
    "## Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "571869e7-09f9-429b-8345-ec5989d7914b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import lief\n",
    "import hashlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "import sys \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65ed0816",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9ea1d07-546d-468a-a9ca-595468576416",
   "metadata": {},
   "outputs": [],
   "source": [
    "LIEF_MAJOR, LIEF_MINOR, _ = lief.__version__.split('.')\n",
    "LIEF_EXPORT_OBJECT = int(LIEF_MAJOR) > 0 or ( int(LIEF_MAJOR)==0 and int(LIEF_MINOR) >= 10 )\n",
    "LIEF_HAS_SIGNATURE = int(LIEF_MAJOR) > 0 or (int(LIEF_MAJOR) == 0 and int(LIEF_MINOR) >= 11)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cab8bc1-ee50-42b9-b8dc-e2f42097e601",
   "metadata": {},
   "source": [
    "## Base class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f443233e-a6eb-4b8e-bbeb-e0fa0ecd028c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"base class which other class will inherit\"\n",
    "class FeatureType(object):\n",
    "\n",
    "    name = ''\n",
    "    dim = 0\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}({})'.format(self.name, self.dim)\n",
    "\n",
    "    def raw_features(self, bytez, lief_binary):\n",
    "        \"from pe to json\"\n",
    "        raise (NotImplementedError)\n",
    "\n",
    "    def process_raw_features(self, raw_obj):\n",
    "        \"form json to vector\"\n",
    "        raise (NotImplementedError)\n",
    "\n",
    "    def feature_vector(self, bytez, lief_binary):\n",
    "        \"from PE to vector\"\n",
    "        return self.process_raw_features(self.raw_features(bytez, lief_binary))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc82c19-bfb2-4cf3-a2d8-dd8afe5e9001",
   "metadata": {},
   "source": [
    "## Byte histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72aea46b-1ec8-41b6-970f-3f6207125511",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ByteHistogram(FeatureType):\n",
    "\n",
    "    name = 'histogram'\n",
    "    dim = 256\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FeatureType, self).__init__()\n",
    "\n",
    "    def raw_features(self, bytez, lief_binary):\n",
    "        counts = np.bincount(np.frombuffer(bytez, dtype=np.uint8), minlength=256)\n",
    "        return counts.tolist()\n",
    "\n",
    "    def process_raw_features(self, raw_obj):\n",
    "        counts = np.array(raw_obj, dtype=np.float32)\n",
    "        sum = counts.sum()\n",
    "        normalized = counts / sum\n",
    "        return normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3444e377-7f85-49fd-a099-e5176ab2b6ce",
   "metadata": {},
   "source": [
    "## Byte Entropy Histogram\n",
    "####  based loosely on (Saxe and Berlin, 2015) [link](https://arxiv.org/pdf/1508.03096.pdf).\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8cea81a-264b-4dc2-88e3-ec6923d63859",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ByteEntropyHistogram(FeatureType):\n",
    "\n",
    "    name = 'byteentropy'\n",
    "    dim = 256\n",
    "\n",
    "    def __init__(self, step=1024, window=2048):\n",
    "        super(FeatureType, self).__init__()\n",
    "        self.window = window\n",
    "        self.step = step\n",
    "\n",
    "    def _entropy_bin_counts(self, block):\n",
    "        c = np.bincount(block >> 4, minlength=16)\n",
    "        p = c.astype(np.float32) / self.window\n",
    "        wh = np.where(c)[0]\n",
    "        H = np.sum(-p[wh] * np.log2(p[wh])) * 2\n",
    "        Hbin = int(H * 2)\n",
    "        if Hbin == 16:\n",
    "            Hbin = 15\n",
    "        return Hbin, c\n",
    "\n",
    "    def raw_features(self, bytez, lief_binary):\n",
    "        output = np.zeros((16, 16), dtype=int)\n",
    "        a = np.frombuffer(bytez, dtype=np.uint8)\n",
    "        if a.shape[0] < self.window:\n",
    "            Hbin, c = self._entropy_bin_counts(a)\n",
    "            output[Hbin, :] += c\n",
    "        else:\n",
    "            shape = a.shape[:-1] + (a.shape[-1] - self.window + 1, self.window)\n",
    "            strides = a.strides + (a.strides[-1],)\n",
    "            blocks = np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)[::self.step, :]\n",
    "            for block in blocks:\n",
    "                Hbin, c = self._entropy_bin_counts(block)\n",
    "                output[Hbin, :] += c\n",
    "        return output.flatten().tolist()\n",
    "\n",
    "    def process_raw_features(self, raw_obj):\n",
    "        counts = np.array(raw_obj, dtype=np.float32)\n",
    "        sum = counts.sum()\n",
    "        normalized = counts / sum\n",
    "        return normalized\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f864aa9-74e6-4133-b876-390ac1bdc791",
   "metadata": {},
   "source": [
    "## Extraction the section "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9772b47b-2323-4ca8-808c-910b6ca0bbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SectionInfo(FeatureType):\n",
    "\n",
    "    name = 'section'\n",
    "    dim = 5 + 50 + 50 + 50 + 50 + 50\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FeatureType, self).__init__()\n",
    "\n",
    "    @staticmethod\n",
    "    def _properties(s):\n",
    "        return [str(c).split('.')[-1] for c in s.characteristics_lists]\n",
    "\n",
    "    def raw_features(self, bytez, lief_binary):\n",
    "        if lief_binary is None:\n",
    "            return {\"entry\": \"\",\"sections\": []}\n",
    "\n",
    "        # properties of entry point, or if invalid, the first executable section\n",
    "\n",
    "        try:\n",
    "            if int(LIEF_MAJOR) > 0 or (int(LIEF_MAJOR) == 0 and int(LIEF_MINOR) >= 12):\n",
    "                section = lief_binary.section_from_rva(lief_binary.entrypoint - lief_binary.imagebase)\n",
    "                if section is None:\n",
    "                    raise lief.not_found\n",
    "                entry_section = section.name\n",
    "            else: # lief < 0.12\n",
    "                entry_section = lief_binary.section_from_offset(lief_binary.entrypoint).name\n",
    "        except lief.not_found:\n",
    "                # bad entry point, let's find the first executable section\n",
    "                entry_section = \"\"\n",
    "                for s in lief_binary.sections:\n",
    "                    if lief.PE.SECTION_CHARACTERISTICS.MEM_EXECUTE in s.characteristics_lists:\n",
    "                        entry_section = s.name\n",
    "                        break\n",
    "\n",
    "        raw_obj = {\"entry\": entry_section}\n",
    "        raw_obj[\"sections\"] = [{\n",
    "            'name': s.name,\n",
    "            'size': s.size,\n",
    "            'entropy': s.entropy,\n",
    "            'vsize': s.virtual_size,\n",
    "            'props': self._properties(s)\n",
    "        } for s in lief_binary.sections]\n",
    "        return raw_obj\n",
    "\n",
    "    def process_raw_features(self, raw_obj):\n",
    "        sections = raw_obj['sections']\n",
    "        general = [\n",
    "            len(sections),  # total number of sections\n",
    "            # number of sections with zero size\n",
    "            sum(1 for s in sections if s['size'] == 0),\n",
    "            # number of sections with an empty name\n",
    "            sum(1 for s in sections if s['name'] == \"\"),\n",
    "            # number of RX\n",
    "            sum(1 for s in sections if 'MEM_READ' in s['props'] and 'MEM_EXECUTE' in s['props']),\n",
    "            # number of W\n",
    "            sum(1 for s in sections if 'MEM_WRITE' in s['props'])\n",
    "        ]\n",
    "        # gross characteristics of each section\n",
    "        section_sizes = [(s['name'], s['size']) for s in sections]\n",
    "        section_sizes_hashed = FeatureHasher(50, input_type=\"pair\").transform([section_sizes]).toarray()[0]\n",
    "        section_entropy = [(s['name'], s['entropy']) for s in sections]\n",
    "        section_entropy_hashed = FeatureHasher(50, input_type=\"pair\").transform([section_entropy]).toarray()[0]\n",
    "        section_vsize = [(s['name'], s['vsize']) for s in sections]\n",
    "        section_vsize_hashed = FeatureHasher(50, input_type=\"pair\").transform([section_vsize]).toarray()[0]\n",
    "        entry_name_hashed = FeatureHasher(50, input_type=\"string\").transform([[raw_obj['entry']]]).toarray()[0]\n",
    "        characteristics = [p for s in sections for p in s['props'] if s['name'] == raw_obj['entry']]\n",
    "        characteristics_hashed = FeatureHasher(50, input_type=\"string\").transform([characteristics]).toarray()[0]\n",
    "\n",
    "        return np.hstack([\n",
    "            general, section_sizes_hashed, section_entropy_hashed, section_vsize_hashed, entry_name_hashed,\n",
    "            characteristics_hashed\n",
    "        ]).astype(np.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0c26cd-81f3-4079-88b8-4e9cc62dbd7f",
   "metadata": {},
   "source": [
    "## Extracting the imports (Library+Functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "870010f7-1316-458a-9ced-36569d315d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImportsInfo(FeatureType):\n",
    "\n",
    "    name = 'imports'\n",
    "    dim = 1280\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FeatureType, self).__init__()\n",
    "\n",
    "    def raw_features(self, bytez, lief_binary):\n",
    "        imports = {}\n",
    "        if lief_binary is None:\n",
    "            return imports\n",
    "\n",
    "        for lib in lief_binary.imports:\n",
    "            if lib.name not in imports:\n",
    "                imports[lib.name] = []  # libraries can be duplicated in listing, extend instead of overwrite\n",
    "\n",
    "            for entry in lib.entries:\n",
    "                if entry.is_ordinal:\n",
    "                    imports[lib.name].append(\"ordinal\" + str(entry.ordinal))\n",
    "                else:\n",
    "                    imports[lib.name].append(entry.name[:10000])\n",
    "\n",
    "        return imports\n",
    "\n",
    "    def process_raw_features(self, raw_obj):\n",
    "        # unique libraries\n",
    "        libraries = list(set([l.lower() for l in raw_obj.keys()]))\n",
    "        libraries_hashed = FeatureHasher(256, input_type=\"string\").transform([libraries]).toarray()[0]\n",
    "\n",
    "        # A string like \"kernel32.dll:CreateFileMappingA\" for each imported function\n",
    "        imports = [lib.lower() + ':' + e for lib, elist in raw_obj.items() for e in elist]\n",
    "        imports_hashed = FeatureHasher(1024, input_type=\"string\").transform([imports]).toarray()[0]\n",
    "\n",
    "        # Two separate elements: libraries (alone) and fully-qualified names of imported functions\n",
    "        return np.hstack([libraries_hashed, imports_hashed]).astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d7651d-8d28-46f4-b552-fa38a05d0f4d",
   "metadata": {},
   "source": [
    "## Extracting the exports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a4ba74e-b062-4192-a813-b11483061678",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExportsInfo(FeatureType):\n",
    "\n",
    "    name = 'exports'\n",
    "    dim = 128\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FeatureType, self).__init__()\n",
    "\n",
    "    def raw_features(self, bytez, lief_binary):\n",
    "        if lief_binary is None:\n",
    "            return []\n",
    "\n",
    "        if LIEF_EXPORT_OBJECT:\n",
    "            # export is an object with .name attribute (0.10.0 and later)\n",
    "            clipped_exports = [export.name[:10000] for export in lief_binary.exported_functions]\n",
    "        else:\n",
    "            # export is a string (LIEF 0.9.0 and earlier)\n",
    "            clipped_exports = [export[:10000] for export in lief_binary.exported_functions]\n",
    "\n",
    "\n",
    "        return clipped_exports\n",
    "\n",
    "    def process_raw_features(self, raw_obj):\n",
    "        exports_hashed = FeatureHasher(128, input_type=\"string\").transform([raw_obj]).toarray()[0]\n",
    "        return exports_hashed.astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40821db1-fd31-4b8f-afad-226736ee3275",
   "metadata": {},
   "source": [
    "## Genral infromation about the PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6efb662-defc-47e6-9b48-146cbd02769d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneralFileInfo(FeatureType):\n",
    "    \n",
    "\n",
    "    name = 'general'\n",
    "    dim = 10\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FeatureType, self).__init__()\n",
    "\n",
    "    def raw_features(self, bytez, lief_binary):\n",
    "        if lief_binary is None:\n",
    "            return {\n",
    "                'size': len(bytez),\n",
    "                'vsize': 0,\n",
    "                'has_debug': 0,\n",
    "                'exports': 0,\n",
    "                'imports': 0,\n",
    "                'has_relocations': 0,\n",
    "                'has_resources': 0,\n",
    "                'has_signature': 0,\n",
    "                'has_tls': 0,\n",
    "                'symbols': 0\n",
    "            }\n",
    "\n",
    "        return {\n",
    "            'size': len(bytez),\n",
    "            'vsize': lief_binary.virtual_size,\n",
    "            'has_debug': int(lief_binary.has_debug),\n",
    "            'exports': len(lief_binary.exported_functions),\n",
    "            'imports': len(lief_binary.imported_functions),\n",
    "            'has_relocations': int(lief_binary.has_relocations),\n",
    "            'has_resources': int(lief_binary.has_resources),\n",
    "            'has_signature': int(lief_binary.has_signatures) if LIEF_HAS_SIGNATURE else int(lief_binary.has_signature),\n",
    "            'has_tls': int(lief_binary.has_tls),\n",
    "            'symbols': len(lief_binary.symbols),\n",
    "        }\n",
    "\n",
    "    def process_raw_features(self, raw_obj):\n",
    "        return np.asarray([\n",
    "            raw_obj['size'], raw_obj['vsize'], raw_obj['has_debug'], raw_obj['exports'], raw_obj['imports'],\n",
    "            raw_obj['has_relocations'], raw_obj['has_resources'], raw_obj['has_signature'], raw_obj['has_tls'],\n",
    "            raw_obj['symbols']\n",
    "        ],dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b78a26-e0b3-458c-b46a-d0fbb040c195",
   "metadata": {},
   "source": [
    "## Extracting the header (coff + optional )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34197476-7dd1-4418-bde7-4818893f1d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeaderFileInfo(FeatureType):\n",
    "\n",
    "    name = 'header'\n",
    "    dim = 62\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FeatureType, self).__init__()\n",
    "\n",
    "    def raw_features(self, bytez, lief_binary):\n",
    "        raw_obj = {}\n",
    "        raw_obj['coff'] = {'timestamp': 0, 'machine': \"\", 'characteristics': []}\n",
    "        raw_obj['optional'] = {\n",
    "            'subsystem': \"\",\n",
    "            'dll_characteristics': [],\n",
    "            'magic': \"\",\n",
    "            'major_image_version': 0,\n",
    "            'minor_image_version': 0,\n",
    "            'major_linker_version': 0,\n",
    "            'minor_linker_version': 0,\n",
    "            'major_operating_system_version': 0,\n",
    "            'minor_operating_system_version': 0,\n",
    "            'major_subsystem_version': 0,\n",
    "            'minor_subsystem_version': 0,\n",
    "            'sizeof_code': 0,\n",
    "            'sizeof_headers': 0,\n",
    "            'sizeof_heap_commit': 0\n",
    "        }\n",
    "        if lief_binary is None:\n",
    "            return raw_obj\n",
    "\n",
    "        raw_obj['coff']['timestamp'] = lief_binary.header.time_date_stamps\n",
    "        raw_obj['coff']['machine'] = str(lief_binary.header.machine).split('.')[-1]\n",
    "        raw_obj['coff']['characteristics'] = [str(c).split('.')[-1] for c in lief_binary.header.characteristics_list]\n",
    "        raw_obj['optional']['subsystem'] = str(lief_binary.optional_header.subsystem).split('.')[-1]\n",
    "        raw_obj['optional']['dll_characteristics'] = [\n",
    "            str(c).split('.')[-1] for c in lief_binary.optional_header.dll_characteristics_lists\n",
    "        ]\n",
    "        raw_obj['optional']['magic'] = str(lief_binary.optional_header.magic).split('.')[-1]\n",
    "        raw_obj['optional']['major_image_version'] = lief_binary.optional_header.major_image_version\n",
    "        raw_obj['optional']['minor_image_version'] = lief_binary.optional_header.minor_image_version\n",
    "        raw_obj['optional']['major_linker_version'] = lief_binary.optional_header.major_linker_version\n",
    "        raw_obj['optional']['minor_linker_version'] = lief_binary.optional_header.minor_linker_version\n",
    "        raw_obj['optional'][\n",
    "            'major_operating_system_version'] = lief_binary.optional_header.major_operating_system_version\n",
    "        raw_obj['optional'][\n",
    "            'minor_operating_system_version'] = lief_binary.optional_header.minor_operating_system_version\n",
    "        raw_obj['optional']['major_subsystem_version'] = lief_binary.optional_header.major_subsystem_version\n",
    "        raw_obj['optional']['minor_subsystem_version'] = lief_binary.optional_header.minor_subsystem_version\n",
    "        raw_obj['optional']['sizeof_code'] = lief_binary.optional_header.sizeof_code\n",
    "        raw_obj['optional']['sizeof_headers'] = lief_binary.optional_header.sizeof_headers\n",
    "        raw_obj['optional']['sizeof_heap_commit'] = lief_binary.optional_header.sizeof_heap_commit\n",
    "        return raw_obj\n",
    "\n",
    "    def process_raw_features(self, raw_obj):\n",
    "        return np.hstack([\n",
    "            raw_obj['coff']['timestamp'],\n",
    "            FeatureHasher(10, input_type=\"string\").transform([[raw_obj['coff']['machine']]]).toarray()[0],\n",
    "            FeatureHasher(10, input_type=\"string\").transform([raw_obj['coff']['characteristics']]).toarray()[0],\n",
    "            FeatureHasher(10, input_type=\"string\").transform([[raw_obj['optional']['subsystem']]]).toarray()[0],\n",
    "            FeatureHasher(10, input_type=\"string\").transform([raw_obj['optional']['dll_characteristics']]).toarray()[0],\n",
    "            FeatureHasher(10, input_type=\"string\").transform([[raw_obj['optional']['magic']]]).toarray()[0],\n",
    "            raw_obj['optional']['major_image_version'],\n",
    "            raw_obj['optional']['minor_image_version'],\n",
    "            raw_obj['optional']['major_linker_version'],\n",
    "            raw_obj['optional']['minor_linker_version'],\n",
    "            raw_obj['optional']['major_operating_system_version'],\n",
    "            raw_obj['optional']['minor_operating_system_version'],\n",
    "            raw_obj['optional']['major_subsystem_version'],\n",
    "            raw_obj['optional']['minor_subsystem_version'],\n",
    "            raw_obj['optional']['sizeof_code'],\n",
    "            raw_obj['optional']['sizeof_headers'],\n",
    "            raw_obj['optional']['sizeof_heap_commit'],\n",
    "        ]).astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b544eb-fc81-494a-930c-effcea724ecf",
   "metadata": {},
   "source": [
    "## Extracting strings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d145163-a170-48d1-b538-dfb20d0ad073",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StringExtractor(FeatureType):\n",
    "\n",
    "    name = 'strings'\n",
    "    dim = 1 + 1 + 1 + 96 + 1 + 1 + 1 + 1 + 1\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FeatureType, self).__init__()\n",
    "        # all consecutive runs of 0x20 - 0x7f that are 5+ characters\n",
    "        self._allstrings = re.compile(b'[\\x20-\\x7f]{5,}')\n",
    "        # occurances of the string 'C:\\'.  Not actually extracting the path\n",
    "        self._paths = re.compile(b'c:\\\\\\\\', re.IGNORECASE)\n",
    "        # occurances of http:// or https://.  Not actually extracting the URLs\n",
    "        self._urls = re.compile(b'https?://', re.IGNORECASE)\n",
    "        # occurances of the string prefix HKEY_.  No actually extracting registry names\n",
    "        self._registry = re.compile(b'HKEY_')\n",
    "        # crude evidence of an MZ header (dropper?) somewhere in the byte stream\n",
    "        self._mz = re.compile(b'MZ')\n",
    "\n",
    "    def raw_features(self, bytez, lief_binary):\n",
    "        allstrings = self._allstrings.findall(bytez)\n",
    "        if allstrings:\n",
    "            # statistics about strings:\n",
    "            string_lengths = [len(s) for s in allstrings]\n",
    "            avlength = sum(string_lengths) / len(string_lengths)\n",
    "            # map printable characters 0x20 - 0x7f to an int array consisting of 0-95, inclusive\n",
    "            as_shifted_string = [b - ord(b'\\x20') for b in b''.join(allstrings)]\n",
    "            c = np.bincount(as_shifted_string, minlength=96)  # histogram count\n",
    "            # distribution of characters in printable strings\n",
    "            csum = c.sum()\n",
    "            p = c.astype(np.float32) / csum\n",
    "            wh = np.where(c)[0]\n",
    "            H = np.sum(-p[wh] * np.log2(p[wh]))  # entropy\n",
    "        else:\n",
    "            avlength = 0\n",
    "            c = np.zeros((96,), dtype=np.float32)\n",
    "            H = 0\n",
    "            csum = 0\n",
    "\n",
    "        return {\n",
    "            'numstrings': len(allstrings),\n",
    "            'avlength': avlength,\n",
    "            'printabledist': c.tolist(),  # store non-normalized histogram\n",
    "            'printables': int(csum),\n",
    "            'entropy': float(H),\n",
    "            'paths': len(self._paths.findall(bytez)),\n",
    "            'urls': len(self._urls.findall(bytez)),\n",
    "            'registry': len(self._registry.findall(bytez)),\n",
    "            'MZ': len(self._mz.findall(bytez))\n",
    "        }\n",
    "\n",
    "    def process_raw_features(self, raw_obj):\n",
    "        hist_divisor = float(raw_obj['printables']) if raw_obj['printables'] > 0 else 1.0\n",
    "        return np.hstack([\n",
    "            raw_obj['numstrings'], raw_obj['avlength'], raw_obj['printables'],\n",
    "            np.asarray(raw_obj['printabledist']) / hist_divisor, raw_obj['entropy'], raw_obj['paths'], raw_obj['urls'],\n",
    "            raw_obj['registry'], raw_obj['MZ']\n",
    "        ]).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffea2253-ffd3-411b-b14a-32cfe73131cb",
   "metadata": {},
   "source": [
    "## Extracting information about Data Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81feb087-531e-4045-a7d7-e224997567a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataDirectories(FeatureType):\n",
    "    ''' Extracts size and virtual address of the first 15 data directories '''\n",
    "\n",
    "    name = 'datadirectories'\n",
    "    dim = 15 * 2\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FeatureType, self).__init__()\n",
    "        self._name_order = [\n",
    "            \"EXPORT_TABLE\", \"IMPORT_TABLE\", \"RESOURCE_TABLE\", \"EXCEPTION_TABLE\", \"CERTIFICATE_TABLE\",\n",
    "            \"BASE_RELOCATION_TABLE\", \"DEBUG\", \"ARCHITECTURE\", \"GLOBAL_PTR\", \"TLS_TABLE\", \"LOAD_CONFIG_TABLE\",\n",
    "            \"BOUND_IMPORT\", \"IAT\", \"DELAY_IMPORT_DESCRIPTOR\", \"CLR_RUNTIME_HEADER\"\n",
    "        ]\n",
    "\n",
    "    def raw_features(self, bytez, lief_binary):\n",
    "        output = []\n",
    "        if lief_binary is None:\n",
    "            return output\n",
    "\n",
    "        for data_directory in lief_binary.data_directories:\n",
    "            output.append({\n",
    "                \"name\": str(data_directory.type).replace(\"DATA_DIRECTORY.\", \"\"),\n",
    "                \"size\": data_directory.size,\n",
    "                \"virtual_address\": data_directory.rva\n",
    "            })\n",
    "        return output\n",
    "\n",
    "    def process_raw_features(self, raw_obj):\n",
    "        features = np.zeros(2 * len(self._name_order), dtype=np.float32)\n",
    "        for i in range(len(self._name_order)):\n",
    "            if i < len(raw_obj):\n",
    "                features[2 * i] = raw_obj[i][\"size\"]\n",
    "                features[2 * i + 1] = raw_obj[i][\"virtual_address\"]\n",
    "        return features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07b0f4c-a382-48c5-800e-328b3e480bb7",
   "metadata": {},
   "source": [
    "## Extraction class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "20b05b07-2016-45d4-b66f-214f6e9991ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PEFeatureExtractor(object):\n",
    "    ''' Extract useful features from a PE file, and return as a vector of fixed size. '''\n",
    "\n",
    "    def __init__(self,feature_list=0, print_feature_warning=True, features_file=''):\n",
    "        self.features = []\n",
    "        features = {\n",
    "                    'ByteHistogram': ByteHistogram(),\n",
    "                    'ByteEntropyHistogram': ByteEntropyHistogram(),\n",
    "                    'StringExtractor': StringExtractor(),\n",
    "                    'GeneralFileInfo': GeneralFileInfo(),\n",
    "                    'HeaderFileInfo': HeaderFileInfo(),\n",
    "                    'SectionInfo': SectionInfo(),\n",
    "                    'ImportsInfo': ImportsInfo(),\n",
    "                    'ExportsInfo': ExportsInfo(),\n",
    "                    'DataDirectories':DataDirectories()\n",
    "            }\n",
    "\n",
    "        if os.path.exists(features_file):\n",
    "            with open(features_file, encoding='utf8') as f:\n",
    "                x = json.load(f)\n",
    "                self.features = [features[feature] for feature in x['features'] if feature in features]\n",
    "        else:\n",
    "            if feature_list :\n",
    "                self.features=[features[fe] for fe in feature_list ]\n",
    "            else :\n",
    "                self.features = list(features.values())\n",
    "\n",
    "\n",
    "        \n",
    "        self.dim = sum([fe.dim for fe in self.features])\n",
    "\n",
    "    def raw_features(self, bytez):\n",
    "        lief_errors = (lief.bad_format, lief.bad_file, lief.pe_error, lief.parser_error, lief.read_out_of_bound,\n",
    "                       RuntimeError)\n",
    "        try:\n",
    "            lief_binary = lief.PE.parse(list(bytez))\n",
    "        except lief_errors as e:\n",
    "            print(\"lief error: \", str(e))\n",
    "            lief_binary = None\n",
    "        except Exception:  # everything else (KeyboardInterrupt, SystemExit, ValueError):\n",
    "            raise\n",
    "\n",
    "        features = {\"sha256\": hashlib.sha256(bytez).hexdigest()}\n",
    "        features.update({fe.name: fe.raw_features(bytez, lief_binary) for fe in self.features})\n",
    "        return features\n",
    "\n",
    "    def process_raw_features(self, raw_obj):\n",
    "        feature_vectors = [fe.process_raw_features(raw_obj[fe.name]) for fe in self.features]\n",
    "        return np.hstack(feature_vectors).astype(np.float32)\n",
    "\n",
    "    def feature_vector(self, bytez):\n",
    "        return self.process_raw_features(self.raw_features(bytez))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af58bb1e-d011-4d8f-91c7-36b3ed2ea1d8",
   "metadata": {},
   "source": [
    "## Extracting and predict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ccc56b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "21f82c89-3e8f-4a7f-98bc-a145e68a2631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.]\n"
     ]
    }
   ],
   "source": [
    "pe_file_path = r\"D:\\Donwlodes\\JavaSetup8u361.exe\"\n",
    "\n",
    "# Create an instance of PEFeatureExtractor\n",
    "feature_extractor = PEFeatureExtractor()\n",
    "\n",
    "# Read the contents of the PE file\n",
    "with open(pe_file_path, 'rb') as f:\n",
    "    bytez = f.read()\n",
    "\n",
    "# Extract features from the PE file\n",
    "try:\n",
    "   feature_vector = feature_extractor.feature_vector(bytez)\n",
    "except :\n",
    "    traceback.print_exc()\n",
    "sample= np.array(feature_vector).reshape(1,-1)\n",
    "    \n",
    "loaded_model = xgb.Booster()\n",
    "loaded_model.load_model('XGB_fold0.xgb')\n",
    "dtest=xgb.DMatrix(sample)\n",
    "\n",
    "y_pred = loaded_model.predict(dtest)\n",
    "\n",
    "# Convert predicted probabilities to class labels\n",
    "y_pred_labels = np.round(y_pred)\n",
    "\n",
    "# Calculate accuracy\n",
    "print(y_pred_labels)\n",
    "\n",
    "\n",
    "\n",
    "# with open(\"my.json\", \"w\") as json_file:\n",
    "#     # Convert dictionary to JSON and write to the file\n",
    "#     json.dump(feature_vector.tolist(), json_file)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07e1683",
   "metadata": {},
   "source": [
    "## Procees the row files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f80caf8-52d6-42cf-a8ed-0cf6df516970",
   "metadata": {},
   "outputs": [],
   "source": [
    "index=0\n",
    "def iterat_file(paths):\n",
    "     for path in paths:  \n",
    "        with open(path,\"r\") as f:\n",
    "            for line in f:\n",
    "                yield line.strip()  \n",
    "def vector_processing(x_path,y_path,extractor,sample,nrow):\n",
    "        global index\n",
    "        sample_object=eval(sample)\n",
    "        x=np.memmap(x_path, dtype='float32', mode='r+', shape=(nrow,extractor.dim))\n",
    "        y=np.memmap(y_path, dtype='float32', mode='r+', shape=nrow)\n",
    "        \n",
    "        \n",
    "        if sample_object[\"label\"]==0 or sample_object[\"label\"]==1 :\n",
    "           x[index]=extractor.process_raw_features(sample_object)\n",
    "           y[index]=sample_object[\"label\"]\n",
    "           index+=1\n",
    "        del x,y\n",
    "def vectorize_arrg(args):\n",
    "        return vector_processing(*args)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0b6557",
   "metadata": {},
   "source": [
    "## genrating the traing vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6b5d3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDirectories=r\"D:\\ember_dataset_2018_2.tar\\ember_dataset_2018_2\\ember2018\"\n",
    "paths=[os.path.join(dataDirectories,f\"train_features_{n}.jsonl\") for n in range(6)]\n",
    "number_sample_train=0\n",
    "for path in paths:\n",
    "    with open(path,\"r\") as file:\n",
    "            for line in file:\n",
    "                line = line.strip()\n",
    "                line=eval(line)\n",
    "                if line[\"label\"]==0 or line[\"label\"]==1:\n",
    "                    number_sample_train+=1\n",
    "diroctorie=r\"D:\\project_2CS\"\n",
    "x_path=os.path.join(diroctorie,\"x_train_all.data\")\n",
    "y_path=os.path.join(diroctorie,\"y_train_all.data\")\n",
    "y=np.memmap(y_path, dtype='float32', mode='w+', shape=number_sample_train)\n",
    "x=np.memmap(x_path, dtype='float32', mode='w+', shape=(number_sample_train,PEFeatureExtractor().dim))\n",
    "del x,y\n",
    "iterator=((x_path,y_path,PEFeatureExtractor(),line,number_sample_train)for line in iterat_file(paths))\n",
    "for iter in tqdm(iterator,total=number_sample_train):\n",
    "    vectorize_arrg(iter)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a6049e",
   "metadata": {},
   "source": [
    "## generating test vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c20b8261",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDirectories=r\"D:\\ember_dataset_2018_2.tar\\ember_dataset_2018_2\\ember2018\"\n",
    "paths=[os.path.join(dataDirectories,\"test_features.jsonl\")]\n",
    "number_sample=sum([1  for f in paths for line in open(f)])\n",
    "diroctorie=r\"C:\\Users\\serra\\Desktop\\project_2CS\"\n",
    "x_path=os.path.join(diroctorie,\"x_test.data\")\n",
    "y_path=os.path.join(diroctorie,\"y_test.data\")\n",
    "y=np.memmap(y_path, dtype='float32', mode='w+', shape=number_sample)\n",
    "x=np.memmap(x_path, dtype='float32', mode='w+', shape=(number_sample,PEFeatureExtractor().dim))\n",
    "del x,y\n",
    "iterator=((x_path,y_path,PEFeatureExtractor(),line,number_sample_train)for line in iterat_file(paths))\n",
    "for iter in tqdm(iterator,total=number_sample):\n",
    "    vectorize_arrg(iter)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9d8fba",
   "metadata": {},
   "source": [
    "## Csv convertion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "129e9c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=np.memmap(r\"D:\\project_2CS\\x_train.data\", dtype='float32', mode='r', shape=(number_sample_train,PEFeatureExtractor().dim))\n",
    "y_train=np.memmap(r\"D:\\project_2CS\\y_train.data\", dtype='float32', mode='r', shape=number_sample_train)\n",
    "x_test=np.memmap(r\"D:\\project_2CS\\x_test.data\", dtype='float32', mode='r', shape=(number_sample,PEFeatureExtractor().dim))\n",
    "y_test=np.memmap(r\"D:\\project_2CS\\y_test.data\", dtype='float32', mode='r', shape=number_sample)\n",
    "df_x_train=pd.DataFrame(x_train)\n",
    "df_y_train=pd.DataFrame(y_train)\n",
    "df_x_test=pd.DataFrame(x_test)\n",
    "df_y_test=pd.DataFrame(y_test)\n",
    "df_x_train.to_csv(\"x_train.csv\",index=False)\n",
    "df_y_train.to_csv(\"y_train.csv\",index=False)\n",
    "df_x_test.to_csv(\"x_test.csv\",index=False)\n",
    "df_y_test.to_csv(\"y_test.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f900a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ember",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
